{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63d8a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required Modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2046a3ba",
   "metadata": {},
   "source": [
    "#  Simple Cross Validation (Train_Test_Split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68b14034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(xArray, yArray,ratio=0.7,seed=42):\n",
    "    if len(xArray)==len(yArray):\n",
    "        high=int(len(xArray)*ratio)\n",
    "        np.random.seed(seed)\n",
    "        x_shuff= np.random.permutation(xArray)\n",
    "        np.random.seed(seed)\n",
    "        y_shuff=np.random.permutation(yArray)\n",
    "        xTrain,xTest,yTrain,yTest=x_shuff[:high],x_shuff[high:],y_shuff[:high],y_shuff[high:]\n",
    "        return xTrain,xTest,yTrain,yTest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b1b42",
   "metadata": {},
   "source": [
    "# Mathematics behind Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede11644",
   "metadata": {},
   "source": [
    "<font size= \"3\">Before diving into the code, we must know the meaning and mathematical intuition behind a simple linear regression which involves only one independent variable and one dependent variable. Linear Regression is a statistical technique of finding the best fit line in a dataset. Here, best fit line means the line which gives a certain level of approximation of how the independent variable is changing with the dependent variable. That's it!!</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceba1780",
   "metadata": {},
   "source": [
    "<font size= \"3.5\">There are infinite numbers of line, which can represent the change of independent variable with the dependent one, but we need to find a line which is the most accurate one. To do so, first we need to calculate the error in the lines we have selected, and then the line with the least amount of error is the best fit line. To calculate amount of errors there are different measures, but the most preferred one for a linear regression problem is the '<b>Mean Squared Error</b>'.<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598300e0",
   "metadata": {},
   "source": [
    "<font size=\"3\">Firstly, let's assume a hypothesis function for our best fit line,  ŷ<sub>i</sub> = mx<sub>i</sub> + b <br>\n",
    "Here, ŷ<sub>i</sub> is our prediction of dependent variable, x<sub>i</sub>  is our independent variable, 'm' is the slope of our hypothesis line and 'b' is the y-intercept</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98a4e09",
   "metadata": {},
   "source": [
    "<font size=3>If 'y<sub>i</sub>' is the correct value of dependent variable which is dependent on x<sub>i</sub> and ŷ<sub>i</sub> is the predicted value by our hypothesis, the Mean Squared Error (M.S.E) value for our hypothesis ŷ<sub>i</sub> = mx<sub>i</sub> + b  will be:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05fa53f",
   "metadata": {},
   "source": [
    "<img src=\"Simple_LR_images/MSE_LR.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39df6cb",
   "metadata": {},
   "source": [
    "<font size=\"3\">Let's factorise the <b>M.S.E</b> or cost function which will help us later.<br><br><b>NOTE THIS BEFORE JUMPING INTO FACTORISATION:<br></b><br>1. <font size=\"5\">  &Sigma;x<sub>i</sub> = n x&#772;</font>----------------------(x<sub>1</sub>+ x<sub>2</sub>+ x<sub>3</sub>+ ...+ x<sub>n</sub>)/ n = x&#772;<br>2. <font size=\"5\"> ŷ<sub>i</sub> = mx<sub>i</sub> + b</font> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acfc636",
   "metadata": {},
   "source": [
    "<img src=\"Simple_LR_images/MSE_LR_factorization.jpg\" width='800'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0143d265",
   "metadata": {},
   "source": [
    "<font size=\"3\">We have the mean squared error for our hypothesis line which depends on the slope (m) and y-intercept (b). To get the minimum possible M.S.E the value of slope and y-intercept should be adjusted perfectly. To determine which values of slope and y-intercept has the minimum error, we have to find the local minima of the cost function which depends on 'm' and 'b'. To do so, we have to partially differentiate the M.S.E (cost function) with respect to slope (m) and y-intercept (b). As the M.S.E function has the derivate, 0 at its local minima (in this case, the global minima), we can write:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20db5f",
   "metadata": {},
   "source": [
    "<img src=\"Simple_LR_images/partial_differentiation_zero.jpg\" width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9470641",
   "metadata": {},
   "source": [
    "<font size='3'>So, first let's differentiate the cost function with respect to the y-intercept</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910308fe",
   "metadata": {},
   "source": [
    "<img src=\"Simple_LR_images/partial_diff_wrt_b.jpg\" width='700'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9717ea48",
   "metadata": {},
   "source": [
    "<font size='3'>Now, from above remember that <b> b= y&#772; - m x&#772;</b>, we will substitute this value of 'b' after differentiating M.S.E w.r.t slope 'm' below. So, let's differentiate. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ff0dcd",
   "metadata": {},
   "source": [
    "<img src=\"Simple_LR_images/partial_diff_wrt_m.jpg\" width='700'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46c80ef",
   "metadata": {},
   "source": [
    "<font size='3'>Now, we got the values of both slope and y-intercept for our hypothesis function which optimally minimizes the mean squared error. But most of the people including me prefer to write slope 'm' as:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21367e2",
   "metadata": {},
   "source": [
    "<img src=\"Simple_LR_images/slope_another_form.jpg\" width='700'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7821a28",
   "metadata": {},
   "source": [
    "<font size='3'>Both of the forms are same, you can write any of the forms according to your wish but I personally like to write the later one because it directly involves the correlation coeffecient. Here is the proof that proves both of the forms are same</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c70e644",
   "metadata": {},
   "source": [
    "<img src=\"Simple_LR_images/both_forms_m.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf47e78",
   "metadata": {},
   "source": [
    "# Simple Linear Regression From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bfabba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_LR():\n",
    "    def fit(self, X_train, Y_train):\n",
    "        numerator, denominator=0, 0\n",
    "        for i in range(np.shape(X_train)[0]):\n",
    "            numerator += numerator + (X_train[i]-X_train.mean())*(Y_train[i]-Y_train.mean())\n",
    "            denominator += denominator + (X_train[i]-X_train.mean())**2\n",
    "        self.m = numerator/denominator\n",
    "        self.b = Y_train.mean() - self.m*X_train.mean()\n",
    "        \n",
    "    def predict(self, x_value):\n",
    "        return (self.m)*x_value + self.b\n",
    "    \n",
    "    def mse(self, Y_train, Y_pred):\n",
    "        return sum(((Y_pred-Y_train)**2))/len(Y_train)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e07ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating some sample datasets to test\n",
    "df=sns.load_dataset('tips')\n",
    "X_values=df['total_bill'].values\n",
    "Y_values=df['tip'].values\n",
    "X_train,X_test,y_train, y_test=train_test_split(X_values, Y_values, seed=100)\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.xlabel('Total Bill in $')\n",
    "plt.ylabel('Total tips in $')\n",
    "\n",
    "#Creating a regressor using simple_LR() \n",
    "regressor=simple_LR()\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_train)\n",
    "\n",
    "#Plotting the regression line\n",
    "plt.plot(X_train, y_pred, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfa2e715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0325694461701023"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the Mean Square Error \n",
    "regressor.mse(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c423a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
